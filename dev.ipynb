{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True # False by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFrameDataset(Dataset):\n",
    "    '''Train only'''\n",
    "\n",
    "    def __init__(self, path_to_data, path_to_split, transform=None, seed=13): \n",
    "        self.path_to_data = path_to_data\n",
    "        voice_set_labels = pd.read_table(path_to_split, sep=' ', names=['path', 'phase'])\n",
    "        voice_set_labels.replace({'_000': '/0', '.wav$': ''}, inplace=True, regex=True)\n",
    "        mask = (voice_set_labels.phase == 1) | (voice_set_labels.phase == 3)\n",
    "        dataset = voice_set_labels[mask].reset_index(drop=True)\n",
    "        self.dataset = dataset['path'][:300] # here\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### VISUAL INPUT\n",
    "        video_path = os.path.join(self.path_to_data, 'video', self.dataset[idx] + '.txt')\n",
    "        frames = pd.read_table(video_path, skiprows=6, usecols=['FRAME '])\n",
    "        earliest = frames['FRAME '].iloc[0]\n",
    "        latest = frames['FRAME '].iloc[-1]\n",
    "        frame_list = np.arange(earliest, latest+1)\n",
    "        mask = np.where(frame_list % 25 == 0)\n",
    "        # only 20 per each face-track (see the asterics on the project page)\n",
    "        # frames_sec = frame_list[mask]\n",
    "        frames_sec = frame_list[mask][:20]\n",
    "        selected_frame = np.random.choice(frames_sec)\n",
    "        selected_frame_filename = '{0:07d}.jpg'.format(selected_frame)\n",
    "        selected_frame_path = os.path.join(self.path_to_data, 'video', \n",
    "                                           self.dataset[idx][:-5] + selected_frame_filename)\n",
    "        frame = cv2.cvtColor(cv2.imread(selected_frame_path), cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        ### AUDIO INPUT\n",
    "        audio_path = os.path.join(self.path_to_data, 'audio', self.dataset[idx] + '.wav')\n",
    "        sample_rate, samples = wavfile.read(audio_path)\n",
    "        \n",
    "        ## parameters\n",
    "        segment_len = 3\n",
    "        window = 'hamming'\n",
    "        window_width = int(sample_rate * 0.025)\n",
    "        overlap = int(sample_rate * (0.025 - 0.010))\n",
    "        FFT_len = 2 ** (window_width - 1).bit_length()\n",
    "        pre_emphasis = 0.97\n",
    "        \n",
    "        # preemphasis filter\n",
    "        samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "    \n",
    "        upper_bound = len(samples) - segment_len * sample_rate\n",
    "        start = np.random.randint(0, upper_bound)\n",
    "        end = start + segment_len * sample_rate\n",
    "        # Note, it produces 512x298 and I don't know why there is some subtle\n",
    "        # differences. However, since the model averages second axis it doesn't\n",
    "        # matter from the computational POV.\n",
    "        _, _, spectrogram = signal.spectrogram(samples[start:end], sample_rate, \n",
    "                                               window=window, nfft=FFT_len, \n",
    "                                               nperseg=window_width, noverlap=overlap, \n",
    "                                               mode='magnitude', return_onesided=False)\n",
    "        spectrogram *= sample_rate / 10\n",
    "        \n",
    "        if self.transform:\n",
    "            frame = frame.astype(np.float32)\n",
    "            spectrogram = spectrogram.astype(np.float32)\n",
    "            frame, spectrogram = self.transform((frame, spectrogram))\n",
    "        \n",
    "        return frame, spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes both face (mean) and voice spectrogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, spectrogram = sample\n",
    "\n",
    "        ## FACE (H, W, C)\n",
    "        # mean normalization for every image (not batch)\n",
    "        mu = frame.mean(axis=(0, 1))\n",
    "        frame = frame - mu\n",
    "        \n",
    "        ## VOICE (Freq, Time)\n",
    "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
    "        mu = spectrogram.mean(axis=1).reshape(512, 1)\n",
    "        sigma = spectrogram.std(axis=1).reshape(512, 1)\n",
    "        spectrogram = (spectrogram - mu) / sigma\n",
    "\n",
    "        return frame, spectrogram\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    '''Horizontally flip the given Image ndarray randomly with a given probability.'''\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, spectrogram = sample\n",
    "\n",
    "        if random.random() < self.p:\n",
    "            return cv2.flip(frame, 1), spectrogram\n",
    "        \n",
    "        return frame, spectrogram\n",
    "\n",
    "class ColorJittering(object):\n",
    "    '''Given Image ndarray performs brightness and \n",
    "    saturation jittering. It is not mentioned in the paper but I guess \n",
    "    the authors used MatConvNet but do not mention any specific augmentation\n",
    "    parameters. So, I made my wind guess regarding the parameters and implemented\n",
    "    augmentation in the following fashion as in:\n",
    "    http://www.vlfeat.org/matconvnet/mfiles/vl_imreadjpeg/\n",
    "    and the Section 3.5 of the manual\n",
    "    http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf'''\n",
    "    \n",
    "    def __init__(self, brightness=[255/25.5, 255/25.5, 255/25.5], saturation=0.5):\n",
    "        # brightness\n",
    "        self.B = np.array(brightness, dtype=np.float32)\n",
    "        # saturation\n",
    "        self.S = saturation\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, spectrogram = sample\n",
    "        \n",
    "        # brightness\n",
    "        # gives an error w/o float32 -- normal() returns float64\n",
    "        w = np.float32(np.random.normal(size=3))\n",
    "        b = self.B * w\n",
    "        frame = np.clip(frame + b, 0, 255)\n",
    "        \n",
    "        # saturation\n",
    "        sigma = np.random.uniform(1-self.S, 1+self.S)\n",
    "        frame = sigma * frame + (1-sigma) / 3 * frame.sum(axis=2, keepdims=True)\n",
    "        frame = np.clip(frame, 0, 255)\n",
    "        \n",
    "        return frame, spectrogram\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, spectrogram = sample\n",
    "        F, T = spectrogram.shape\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        frame = frame.transpose((2, 0, 1))\n",
    "        \n",
    "        # now specs are of size (Freq, Time) 2D but has to be 3D\n",
    "        spectrogram = spectrogram.reshape(1, F, T)\n",
    "\n",
    "        return torch.from_numpy(frame), torch.from_numpy(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY TO ADD DROPOUT\n",
    "\n",
    "class FaceSubnet(nn.Module):\n",
    "\n",
    "    def __init__(self, seed=13):\n",
    "        super(FaceSubnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # only after conv layers\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_features=256 * 7 * 7, out_features=4096)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n",
    "        \n",
    "        self.mpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        \n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return F.normalize(x)\n",
    "\n",
    "## TRY TO REMOVE DROPOUT\n",
    "\n",
    "class VoiceSubnet(nn.Module):\n",
    "\n",
    "    def __init__(self, seed=13):\n",
    "        super(VoiceSubnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # only after conv layers\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        \n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.apool6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return F.normalize(x)\n",
    "    \n",
    "class CurriculumMining(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CurriculumMining, self).__init__()\n",
    "        \n",
    "    def forward(self, positive_pairs, tau):\n",
    "        faces, voices = positive_pairs\n",
    "        B, D = faces.size()\n",
    "        # calc dist \n",
    "        # (X - Y) ^ 2 = X^2 + Y^2 - 2XY\n",
    "        x = (faces**2).sum(dim=1).view(-1, 1) + (voices**2).sum(dim=1) - 2*faces.matmul(voices.t())\n",
    "        dists = x.sqrt()\n",
    "        \n",
    "        sorted_dist, sorted_idx = torch.sort(dists, dim=1, descending=True)\n",
    "        Dnj = sorted_dist - dists.diag().view(-1, 1)\n",
    "        idx_threshold = round(tau * (B-1))\n",
    "        \n",
    "        # tricky part\n",
    "        mask = torch.ones_like(sorted_dist)\n",
    "        mask[:, idx_threshold+1:] = 0\n",
    "        mask[Dnj <= 0] = 0\n",
    "        idx_of_sorted_idx = ((mask).sum(dim=1) - 1).abs().long()\n",
    "        neg_samples_idx = torch.gather(sorted_idx, dim=1, index=idx_of_sorted_idx.view(B, 1))\n",
    "        neg_samples_idx = neg_samples_idx.view(B)\n",
    "        \n",
    "        neg_samples_idx = torch.randperm(B) # here\n",
    "\n",
    "        negative_voices = voices[neg_samples_idx]\n",
    "        \n",
    "        return faces, negative_voices\n",
    "    \n",
    "class LearnablePinsNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LearnablePinsNet, self).__init__()\n",
    "        self.face_subnet = FaceSubnet()\n",
    "        self.voice_subnet = VoiceSubnet()\n",
    "        self.curr_mining = CurriculumMining()\n",
    "        \n",
    "    def forward(self, frames, specs, tau=None):\n",
    "        emb_f = self.face_subnet(frames)\n",
    "        emb_v = self.voice_subnet(specs)\n",
    "            \n",
    "        if self.training:\n",
    "            positive_pairs = emb_f, emb_v\n",
    "            negative_pairs = self.curr_mining(positive_pairs, tau)\n",
    "                \n",
    "            return positive_pairs, negative_pairs\n",
    "        \n",
    "        else:\n",
    "            return emb_f, emb_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, positive_pairs, negative_pairs, margin):\n",
    "        ## POSITIVE PART\n",
    "        faces, voices = positive_pairs\n",
    "#         dists_pos = ((faces - voices) ** 2).sum(dim=1).sqrt()\n",
    "#         pos_part = dists_pos ** 2\n",
    "        pos_part = ((faces - voices) ** 2).sum(dim=1)\n",
    "    \n",
    "        ## NEGATIVE PART\n",
    "        faces, voices = negative_pairs\n",
    "        dists_neg = ((faces - voices) ** 2).sum(dim=1).sqrt()\n",
    "        neg_part = (margin - dists_neg).clamp(0) ** 2\n",
    "        \n",
    "        loss4pair = torch.cat([pos_part, neg_part])\n",
    "        \n",
    "        ## CALCULATE LOSS\n",
    "        B, D = faces.size()\n",
    "        batch_loss = loss4pair.sum() / (B + B)\n",
    "    \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TauScheduler(object):\n",
    "    '''\n",
    "    \"found that it was effective to increase \\tau by 10 percent \n",
    "    every two epochs, starting from 30% up until 80%, and keeping \n",
    "    it constant thereafter\"\n",
    "    --- So, it is increasing by 10 % every second epoch:\n",
    "            ⎧tau = tau + tau * 0.1, tau < 0.8, \n",
    "            ⎨\n",
    "            ⎩tau = 0.8, tau > 0.8.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, lowest, highest):\n",
    "        self.current = int(lowest * 100)\n",
    "        self.highest = int(highest * 100)\n",
    "        self.epoch_num = 0\n",
    "\n",
    "    def step(self):\n",
    "            \n",
    "        if self.epoch_num % 2 == 0 and self.epoch_num > 0:\n",
    "#                 self.current += 10\n",
    "            self.current = int(self.current + self.current * 0.1)\n",
    "        \n",
    "        if self.current > self.highest:\n",
    "            self.current = 80\n",
    "    \n",
    "        self.epoch_num += 1\n",
    "        \n",
    "    def get_tau(self):\n",
    "        return np.random.uniform() # here\n",
    "#         return self.current / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = '/home/nvme/logs/LearnablePINs/_overfit_test/'\n",
    "DATA_PATH = '/home/nvme/data/voxceleb1/'\n",
    "SPLIT_PATH = os.path.join(DATA_PATH, 'Splits/filtered_voice_set_labels.txt')\n",
    "FACE_SUBNET_SNAPSHOT_PATH = os.path.join(LOG_PATH, 'face_subnet_snapshot.txt')\n",
    "VOICE_SUBNET_SNAPSHOT_PATH = os.path.join(LOG_PATH, 'voice_subnet_snapshot.txt')\n",
    "DEVICES = [1] # here\n",
    "B = 76 * len(DEVICES)\n",
    "# https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5\n",
    "NUM_WORKERS = 4 * len(DEVICES)\n",
    "MARGIN = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.36 / ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.33it/s]\n",
      "4it [00:02,  1.37it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.46it/s]\n",
      "4it [00:02,  1.46it/s]\n",
      "4it [00:02,  1.38it/s]\n",
      "4it [00:02,  1.44it/s]\n",
      "4it [00:02,  1.41it/s]\n",
      "4it [00:02,  1.39it/s]\n",
      "4it [00:02,  1.39it/s]\n",
      "4it [00:02,  1.40it/s]\n",
      "4it [00:02,  1.67it/s]\n",
      "4it [00:02,  1.20it/s]\n",
      "4it [00:02,  1.19it/s]\n",
      "4it [00:02,  1.19it/s]\n",
      "4it [00:02,  1.16it/s]\n",
      "4it [00:03,  1.09it/s]\n",
      "4it [00:02,  1.24it/s]\n",
      "4it [00:02,  1.13it/s]\n",
      "4it [00:02,  1.22it/s]\n",
      "4it [00:02,  1.20it/s]\n",
      "4it [00:02,  1.17it/s]\n",
      "4it [00:02,  1.15it/s]\n",
      "4it [00:02,  1.18it/s]\n",
      "4it [00:02,  1.13it/s]\n",
      "4it [00:02,  1.21it/s]\n",
      "4it [00:02,  1.16it/s]\n",
      "4it [00:02,  1.11it/s]\n",
      "4it [00:02,  1.15it/s]\n",
      "4it [00:02,  1.18it/s]\n",
      "4it [00:02,  1.38it/s]\n",
      "4it [00:02,  1.41it/s]\n",
      "4it [00:02,  1.37it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.40it/s]\n",
      "4it [00:02,  1.40it/s]\n",
      "4it [00:02,  1.51it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.46it/s]\n",
      "4it [00:02,  1.43it/s]\n",
      "4it [00:02,  1.39it/s]\n",
      "4it [00:02,  1.51it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.40it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.42it/s]\n",
      "4it [00:02,  1.45it/s]\n",
      "4it [00:02,  1.48it/s]\n",
      "4it [00:02,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)\n",
    "\n",
    "transform = Compose([\n",
    "    Normalize(),\n",
    "    RandomHorizontalFlip(),\n",
    "    ColorJittering(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "train = AudioFrameDataset(DATA_PATH, SPLIT_PATH, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=B, num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "net = LearnablePinsNet()\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10 ** (-6/49))\n",
    "tau_scheduler = TauScheduler(lowest=0.3, highest=0.8)\n",
    "eval_results = {}\n",
    "\n",
    "device = \"cuda:1\"\n",
    "torch.cuda.set_device(DEVICES[0])\n",
    "net.to(device);\n",
    "# net = nn.DataParallel(net, DEVICES) # here\n",
    "\n",
    "for epoch_num in range(50):\n",
    "    net.train()\n",
    "    lr_scheduler.step()\n",
    "    tau_scheduler.step()\n",
    "\n",
    "    for iter_num, (frames, specs) in tqdm(enumerate(trainloader)):\n",
    "        # transfer inputs to a device\n",
    "        frames, specs = frames.cuda(async=True), specs.cuda(async=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        positive_pairs, negative_pairs = net(frames, specs, tau=tau_scheduler.get_tau())\n",
    "\n",
    "        loss = criterion(positive_pairs, negative_pairs, margin=MARGIN)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Tensorboard\n",
    "        step_num = epoch_num * len(trainloader) + iter_num\n",
    "        TBoard.add_scalar('Train/Loss', loss.item(), step_num)\n",
    "        TBoard.add_scalar('Train/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "        TBoard.add_scalar('Train/tau', tau_scheduler.get_tau(), step_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
